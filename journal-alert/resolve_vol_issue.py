#!/usr/bin/env python3
# pyright: basic
"""
Notion DBì˜ Vol/Issue ëˆ„ë½ ë…¼ë¬¸ì„ CrossRef + PubMed ì¬ì¡°íšŒë¡œ ë³´ì™„í•©ë‹ˆë‹¤.

Usage:
- python3 resolve_vol_issue.py
- python3 resolve_vol_issue.py --dry-run
- python3 resolve_vol_issue.py --crossref-only
- python3 resolve_vol_issue.py --pubmed-only
"""

import argparse
import glob
import json
import os
import time
import urllib.error
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET


CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")


def load_config() -> dict:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def notion_api(endpoint: str, data: dict, token: str, method: str = "POST") -> dict | None:
    url = f"https://api.notion.com/v1/{endpoint}"
    body = json.dumps(data).encode("utf-8")
    req = urllib.request.Request(
        url,
        data=body,
        method=method,
        headers={
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28",
        },
    )
    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            return json.loads(resp.read())
    except urllib.error.HTTPError as e:
        error_body = e.read().decode()
        print(f"  âŒ Notion API ì˜¤ë¥˜ ({e.code}): {error_body[:200]}")
        return None
    except urllib.error.URLError as e:
        print(f"  âŒ Notion ì—°ê²° ì˜¤ë¥˜: {e}")
        return None


def _extract_rich_text(props: dict, key: str) -> str:
    parts = props.get(key, {}).get("rich_text", [])
    if not parts:
        return ""
    return "".join(part.get("plain_text", "") for part in parts).strip()


def query_all_pages(database_id: str, token: str) -> list[dict]:
    pages = []
    has_more = True
    start_cursor = None

    while has_more:
        payload = {"page_size": 100}
        if start_cursor:
            payload["start_cursor"] = start_cursor

        result = notion_api(f"databases/{database_id}/query", payload, token)
        if not result:
            break

        for page in result.get("results", []):
            props = page.get("properties", {})
            title_items = props.get("Title", {}).get("title", [])
            title = title_items[0].get("plain_text", "").strip() if title_items else ""
            doi_url = props.get("DOI", {}).get("url", "") or ""

            pages.append(
                {
                    "page_id": page.get("id", ""),
                    "title": title,
                    "doi_url": doi_url,
                    "vol": _extract_rich_text(props, "Vol"),
                    "issue": _extract_rich_text(props, "Issue"),
                }
            )

        has_more = result.get("has_more", False)
        start_cursor = result.get("next_cursor")

    return pages


def _normalize_title(text: str) -> str:
    return "".join(ch.lower() for ch in text if ch.isalnum())


def _extract_doi(doi_or_url: str) -> str:
    value = (doi_or_url or "").strip()
    if not value:
        return ""
    lower = value.lower()
    if lower.startswith("https://doi.org/"):
        return value[16:]
    if lower.startswith("http://doi.org/"):
        return value[15:]
    return value


def _safe_text(value) -> str:
    if value is None:
        return ""
    return str(value).strip()


def resolve_via_crossref(doi: str) -> tuple[str, str]:
    doi = _extract_doi(doi)
    if not doi:
        return "", ""

    encoded_doi = urllib.parse.quote(doi, safe="")
    url = f"https://api.crossref.org/works/{encoded_doi}"
    req = urllib.request.Request(
        url,
        headers={
            "Accept": "application/json",
            "User-Agent": "JournalAlert/1.0 (mailto:woontak.yuh@gmail.com)",
        },
    )

    try:
        with urllib.request.urlopen(req, timeout=20) as resp:
            payload = json.loads(resp.read())
    except urllib.error.HTTPError as e:
        print(f"    âš ï¸  CrossRef HTTP ì˜¤ë¥˜ ({e.code})")
        return "", ""
    except urllib.error.URLError as e:
        print(f"    âš ï¸  CrossRef ì—°ê²° ì˜¤ë¥˜: {e}")
        return "", ""
    except Exception as e:
        print(f"    âš ï¸  CrossRef íŒŒì‹± ì˜¤ë¥˜: {e}")
        return "", ""

    message = payload.get("message", {}) if isinstance(payload, dict) else {}
    volume = _safe_text(message.get("volume"))
    issue = _safe_text(message.get("issue"))
    return volume, issue


def resolve_via_pubmed(pmid: str) -> tuple[str, str]:
    pmid = (pmid or "").strip()
    if not pmid:
        return "", ""

    query = urllib.parse.urlencode(
        {
            "db": "pubmed",
            "id": pmid,
            "retmode": "xml",
        }
    )
    url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?{query}"

    try:
        with urllib.request.urlopen(url, timeout=20) as resp:
            xml_text = resp.read().decode("utf-8", errors="ignore")
        root = ET.fromstring(xml_text)
    except urllib.error.HTTPError as e:
        print(f"    âš ï¸  PubMed HTTP ì˜¤ë¥˜ ({e.code})")
        return "", ""
    except urllib.error.URLError as e:
        print(f"    âš ï¸  PubMed ì—°ê²° ì˜¤ë¥˜: {e}")
        return "", ""
    except ET.ParseError as e:
        print(f"    âš ï¸  PubMed XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        return "", ""

    volume = (root.findtext(".//JournalIssue/Volume", "") or "").strip()
    issue = (root.findtext(".//JournalIssue/Issue", "") or "").strip()
    return volume, issue


def _load_latest_articles() -> list[dict]:
    files = sorted(glob.glob(os.path.join(DATA_DIR, "*.json")), key=os.path.getmtime)
    if not files:
        return []

    selected_files = []
    all_journal_files = [p for p in files if os.path.basename(p).startswith("all_journals_")]
    if all_journal_files:
        selected_files = [all_journal_files[-1]]
    else:
        by_prefix = {}
        for path in files:
            name = os.path.basename(path)
            prefix = name.split("_202", 1)[0]
            by_prefix[prefix] = path
        selected_files = sorted(by_prefix.values())

    articles = []
    for path in selected_files:
        try:
            with open(path, "r", encoding="utf-8") as f:
                rows = json.load(f)
                if isinstance(rows, list):
                    articles.extend(rows)
        except Exception as e:
            print(f"  âš ï¸  JSON ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(path)} ({e})")

    return articles


def _build_article_maps(articles: list[dict]) -> tuple[dict, dict]:
    by_doi = {}
    by_title = {}

    for article in articles:
        doi_url = (article.get("doi_url") or "").strip()
        doi = (article.get("doi") or "").strip()
        if doi_url:
            by_doi[doi_url.lower()] = article
        if doi:
            by_doi[f"https://doi.org/{doi}".lower()] = article

        title = (article.get("title") or "").strip()
        if title:
            by_title[_normalize_title(title)] = article

    return by_doi, by_title


def _update_page_vol_issue(page_id: str, volume: str, issue: str, token: str) -> bool:
    props = {}
    if volume:
        props["Vol"] = {"rich_text": [{"text": {"content": volume}}]}
    if issue:
        props["Issue"] = {"rich_text": [{"text": {"content": issue}}]}
    if not props:
        return False

    result = notion_api(f"pages/{page_id}", {"properties": props}, token, method="PATCH")
    return result is not None


def resolve_all(
    database_id: str,
    token: str,
    dry_run: bool = False,
    crossref_only: bool = False,
    pubmed_only: bool = False,
):
    print("ğŸ“‹ Notion DB ì¡°íšŒ ì¤‘...")
    pages = query_all_pages(database_id, token)
    print(f"   {len(pages)}ê±´ ì¡°íšŒë¨")

    targets = [p for p in pages if not p.get("vol") or not p.get("issue")]
    print(f"   Vol/Issue ëˆ„ë½: {len(targets)}ê±´")
    if not targets:
        print("âœ… ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì—†ìŒ")
        return

    print("ğŸ“‚ ìµœì‹  JSON ë¡œë“œ ì¤‘...")
    articles = _load_latest_articles()
    print(f"   {len(articles)}ê±´ ë¡œë“œ")
    by_doi, by_title = _build_article_maps(articles)

    total_resolved = 0
    total_updated = 0
    total_failed = 0
    total_skipped = 0

    for idx, page in enumerate(targets, 1):
        try:
            title = page.get("title", "")
            doi_url = page.get("doi_url", "")
            vol_now = page.get("vol", "")
            issue_now = page.get("issue", "")

            article = None
            if doi_url:
                article = by_doi.get(doi_url.lower())
            if not article and title:
                article = by_title.get(_normalize_title(title))

            doi = _extract_doi(doi_url)
            pmid = ""
            if article:
                if not doi:
                    doi = _extract_doi(article.get("doi_url") or article.get("doi") or "")
                pmid = (article.get("pmid") or "").strip()

            print(f"\nğŸ” [{idx}/{len(targets)}] {title[:70]}...")

            found_vol = ""
            found_issue = ""
            source = ""

            if not pubmed_only and doi:
                print(f"    ğŸŒ CrossRef ì¡°íšŒ: {doi}")
                found_vol, found_issue = resolve_via_crossref(doi)
                time.sleep(1.0)
                if found_vol or found_issue:
                    source = "CrossRef"

            if not source and not crossref_only and pmid:
                print(f"    ğŸ§¬ PubMed ì¬ì¡°íšŒ: PMID {pmid}")
                found_vol, found_issue = resolve_via_pubmed(pmid)
                if found_vol or found_issue:
                    source = "PubMed"

            new_vol = vol_now or found_vol
            new_issue = issue_now or found_issue
            if not new_vol and not new_issue:
                total_skipped += 1
                print("    â­ï¸  í•´ìƒ ì‹¤íŒ¨ (CrossRef/PubMed ëª¨ë‘ ê°’ ì—†ìŒ)")
                continue

            if (new_vol == vol_now) and (new_issue == issue_now):
                total_skipped += 1
                print("    â­ï¸  ë³€ê²½ ì—†ìŒ")
                continue

            total_resolved += 1
            print(f"    âœ… í•´ìƒ ì„±ê³µ ({source}): Vol={new_vol or '-'} / Issue={new_issue or '-'}")

            if dry_run:
                print("    ğŸ§ª DRY-RUN: Notion ì—…ë°ì´íŠ¸ ìƒëµ")
                continue

            ok = _update_page_vol_issue(page["page_id"], new_vol, new_issue, token)
            if ok:
                total_updated += 1
                print("    ğŸ’¾ Notion ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                total_failed += 1
                print("    âŒ Notion ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")

            time.sleep(0.5)
        except Exception as e:
            total_failed += 1
            print(f"    âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    print(
        f"\nâœ… ì™„ë£Œ: í•´ìƒ {total_resolved}ê±´, ì—…ë°ì´íŠ¸ {total_updated}ê±´, "
        f"ìŠ¤í‚µ {total_skipped}ê±´, ì˜¤ë¥˜ {total_failed}ê±´"
    )


def main():
    parser = argparse.ArgumentParser(description="CrossRef + PubMed ì¬ì¡°íšŒë¡œ Vol/Issue ë³´ì™„")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ Notion ì—…ë°ì´íŠ¸ ì—†ì´ ê²°ê³¼ë§Œ ì¶œë ¥")
    parser.add_argument("--crossref-only", action="store_true", help="CrossRefë§Œ ì‚¬ìš©")
    parser.add_argument("--pubmed-only", action="store_true", help="PubMedë§Œ ì‚¬ìš©")
    args = parser.parse_args()

    if args.crossref_only and args.pubmed_only:
        print("âŒ --crossref-only ì™€ --pubmed-only ëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return 1

    config = load_config()
    token = os.environ.get("NOTION_TOKEN") or config.get("notion_token", "")
    if not token:
        print("âŒ NOTION_TOKEN í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” config.jsonì— notion_token ì„¤ì • í•„ìš”")
        return 1

    database_id = config.get("notion_database_id", "")
    if not database_id:
        print("âŒ config.jsonì— notion_database_id ì—†ìŒ")
        return 1

    resolve_all(
        database_id=database_id,
        token=token,
        dry_run=args.dry_run,
        crossref_only=args.crossref_only,
        pubmed_only=args.pubmed_only,
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

#!/usr/bin/env python3
"""
PubMed E-utilitiesë¡œ ì €ë„ ë…¼ë¬¸ ìˆ˜ì§‘
Usage: python fetch_papers.py [--journal "Spine J"] [--year 2026] [--days 30]
"""

import urllib.request
import urllib.error
import urllib.parse
import json
import xml.etree.ElementTree as ET
import time
import argparse
import os
import re
import html as html_mod
from datetime import datetime, timedelta

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "data")
PUBMED_BASE = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"

def fetch_abstract_from_doi(doi: str) -> str:
    if not doi:
        return ""

    doi = doi.strip()
    if not doi:
        return ""

    doi_url = f"https://doi.org/{doi}"
    headers = {"User-Agent": "JournalAlert/1.0"}

    final_url = ""
    try:
        req = urllib.request.Request(doi_url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as resp:
            final_url = resp.geturl()
    except Exception as e:
        print(f"  âš  DOI ì´ë™ ì‹¤íŒ¨ ({doi}): {e}")
        return ""
    finally:
        time.sleep(0.5)

    if not final_url:
        return ""

    domain = urllib.parse.urlparse(final_url).netloc.lower()
    if domain.startswith("www."):
        domain = domain[4:]

    if domain != "link.springer.com":
        return ""

    try:
        req = urllib.request.Request(final_url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as resp:
            page_html = resp.read().decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"  âš  DOI ë³¸ë¬¸ ì¡°íšŒ ì‹¤íŒ¨ ({doi}): {e}")
        return ""
    finally:
        time.sleep(0.5)

    m = re.search(r'id="Abs1-content"[^>]*>(.*?)</div>', page_html, re.DOTALL)
    if not m:
        return ""

    # íƒœê·¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ (êµ¬ì¡°í™” abstractì—ì„œ ë¼ë²¨+ë³¸ë¬¸ ì‚¬ì´ ê³µë°± ë³´ì¡´)
    text = re.sub(r"<[^>]+>", " ", m.group(1))
    text = html_mod.unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    # êµ¬ì¡°í™” abstract ë¼ë²¨ í¬ë§·íŒ…: PurposeRobotic â†’ Purpose: Robotic
    text = re.sub(
        r"(Purpose|Background|Objective|Methods?|Results?|Conclusions?|Study Design|Setting|Patients?|Outcome Measures?|Introduction|Discussion|Significance)"
        r"(?=[A-Z])",
        r"\1: ",
        text,
    )
    return text

def load_config():
    with open(CONFIG_PATH, "r") as f:
        return json.load(f)

def search_pubmed(journal_query: str, year: int | None = None, days: int | None = None, retmax: int = 500) -> list[str]:
    """PubMedì—ì„œ ë…¼ë¬¸ ID ê²€ìƒ‰"""
    term = f'"{journal_query}"[journal]'
    if year:
        term += f" AND {year}[pdat]"
    if days:
        mindate = (datetime.now() - timedelta(days=days)).strftime("%Y/%m/%d")
        maxdate = datetime.now().strftime("%Y/%m/%d")
        term += f"&datetype=pdat&mindate={mindate}&maxdate={maxdate}"
    
    url = f"{PUBMED_BASE}/esearch.fcgi?db=pubmed&term={urllib.parse.quote(term)}&retmax={retmax}&retmode=json"
    
    req = urllib.request.Request(url, headers={"User-Agent": "JournalAlert/1.0"})
    with urllib.request.urlopen(req, timeout=30) as resp:
        data = json.loads(resp.read())
    
    ids = data["esearchresult"]["idlist"]
    count = data["esearchresult"]["count"]
    print(f"  ê²€ìƒ‰: {term}")
    print(f"  ê²°ê³¼: {count}ê±´ ì¤‘ {len(ids)}ê±´ ê°€ì ¸ì˜´")
    return ids

def fetch_details(pmids: list[str]) -> list[dict]:
    """PubMedì—ì„œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    articles = []
    batch_size = 50
    
    for i in range(0, len(pmids), batch_size):
        batch = pmids[i:i+batch_size]
        id_str = ",".join(batch)
        url = f"{PUBMED_BASE}/efetch.fcgi?db=pubmed&id={id_str}&retmode=xml"
        
        req = urllib.request.Request(url, headers={"User-Agent": "JournalAlert/1.0"})
        with urllib.request.urlopen(req, timeout=60) as resp:
            xml_data = resp.read()
        
        root = ET.fromstring(xml_data)
        
        for article in root.findall(".//PubmedArticle"):
            try:
                parsed = parse_article(article)
                if parsed:
                    articles.append(parsed)
            except Exception as e:
                pmid = article.findtext(".//PMID", "unknown")
                print(f"  âš  PMID {pmid} íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        if i + batch_size < len(pmids):
            time.sleep(0.4)  # rate limit
    
    return articles

def parse_article(article) -> dict | None:
    """XMLì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ"""
    # PMID
    pmid = article.findtext(".//PMID", "")
    
    # Title
    title_el = article.find(".//ArticleTitle")
    title = "".join(title_el.itertext()).strip() if title_el is not None else ""
    if not title:
        return None
    
    authors = []
    first_affiliation = ""
    for auth in article.findall(".//Author"):
        last = auth.findtext("LastName", "")
        fore = auth.findtext("ForeName", "")
        init = auth.findtext("Initials", "")
        if last:
            authors.append({"last": last, "fore": fore, "initials": init})
            if not first_affiliation:
                aff_el = auth.find(".//AffiliationInfo/Affiliation")
                if aff_el is not None and aff_el.text:
                    first_affiliation = aff_el.text.strip()
    
    author_str = format_authors(authors)
    
    # Abstract
    abstract_parts = []
    for abs_text in article.findall(".//Abstract/AbstractText"):
        label = abs_text.get("Label", "")
        text = "".join(abs_text.itertext()).strip()
        if label:
            abstract_parts.append(f"{label}: {text}")
        else:
            abstract_parts.append(text)
    # DOI
    doi = ""
    for aid in article.findall(".//ArticleId"):
        if aid.get("IdType") == "doi":
            doi = aid.text
            break

    abstract = " ".join(abstract_parts)
    if not abstract and doi:
        abstract = fetch_abstract_from_doi(doi)
        if abstract:
            print(f"  ğŸ“¥ DOI fallback abstract: {pmid} ({len(abstract)} chars)")
    
    # Journal info
    journal = article.findtext(".//Journal/Title", "")
    journal_abbr = article.findtext(".//ISOAbbreviation", "")
    volume = article.findtext(".//Volume", "")
    issue = article.findtext(".//Issue", "")
    pages = article.findtext(".//MedlinePgn", "")
    
    # Publication date
    pub_date = extract_pub_date(article)
    
    # Keywords
    keywords = []
    for kw in article.findall(".//Keyword"):
        if kw.text:
            keywords.append(kw.text.strip())
    
    # MeSH terms
    mesh = []
    for mh in article.findall(".//MeshHeading/DescriptorName"):
        if mh.text:
            mesh.append(mh.text.strip())
    
    # Publication types
    pub_types = []
    for pt in article.findall(".//PublicationType"):
        if pt.text:
            pub_types.append(pt.text.strip())
    
    return {
        "pmid": pmid,
        "title": title,
        "authors": author_str,
        "author_list": authors[:10],
        "affiliation": first_affiliation,
        "abstract": abstract,
        "doi": doi,
        "doi_url": f"https://doi.org/{doi}" if doi else "",
        "journal": journal,
        "journal_abbr": journal_abbr,
        "volume": volume,
        "issue": issue,
        "pages": pages,
        "pub_date": pub_date,
        "keywords": keywords,
        "mesh_terms": mesh,
        "pub_types": pub_types,
        "pubmed_url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
    }

def format_authors(authors: list[dict], max_show: int = 3) -> str:
    """ì €ì ëª©ë¡ í¬ë§·íŒ…"""
    if not authors:
        return ""
    names = [f"{a['last']} {a['initials']}" for a in authors[:max_show]]
    result = ", ".join(names)
    if len(authors) > max_show:
        result += f" et al."
    return result

def extract_pub_date(article) -> str:
    """ì¶œíŒì¼ ì¶”ì¶œ â†’ YYYY-MM-DD or YYYY-MM or YYYY"""
    # ArticleDate (epub) ìš°ì„ 
    for ad in article.findall(".//ArticleDate"):
        y = ad.findtext("Year") or ""
        m = ad.findtext("Month") or ""
        d = ad.findtext("Day") or ""
        if y:
            parts = [y]
            if m: parts.append(m.zfill(2))
            if d: parts.append(d.zfill(2))
            return "-".join(parts)
    
    # PubDate
    for pd in article.findall(".//PubDate"):
        y = pd.findtext("Year") or ""
        m = pd.findtext("Month") or ""
        d = pd.findtext("Day") or ""
        if y:
            parts = [y]
            if m:
                # Monthê°€ "Jan" ê°™ì€ ë¬¸ìì¼ ìˆ˜ ìˆìŒ
                month_map = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06",
                             "Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
                if m in month_map:
                    m = month_map[m]
                m = m.zfill(2)
                parts.append(m)
            if d: parts.append(d.zfill(2))
            return "-".join(parts)
    
    return ""

def save_results(articles: list[dict], journal_key: str, label: str = ""):
    """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{journal_key}_{label}_{timestamp}.json" if label else f"{journal_key}_{timestamp}.json"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    print(f"  ğŸ’¾ ì €ì¥: {filepath} ({len(articles)}ê±´)")
    return filepath

def print_summary(articles: list[dict]):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    if not articles:
        print("  ê²°ê³¼ ì—†ìŒ")
        return
    
    # Issueë³„ ë¶„ë¥˜
    by_issue = {}
    for a in articles:
        iss = a.get("issue", "unknown") or "unknown"
        by_issue.setdefault(iss, []).append(a)
    
    print(f"\n  ğŸ“Š ì´ {len(articles)}í¸")
    for iss in sorted(by_issue.keys()):
        print(f"     Issue {iss}: {len(by_issue[iss])}í¸")
    
    # Article typeë³„
    types = {}
    for a in articles:
        for pt in a.get("pub_types", []):
            types[pt] = types.get(pt, 0) + 1
    if types:
        print(f"  ğŸ“‹ ìœ í˜•: {', '.join(f'{k}({v})' for k,v in sorted(types.items(), key=lambda x:-x[1])[:5])}")

# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(description="PubMed ì €ë„ ë…¼ë¬¸ ìˆ˜ì§‘")
    parser.add_argument("--journal", help="ì €ë„ëª… (PubMed ì•½ì–´)")
    parser.add_argument("--year", type=int, help="ì¶œíŒë…„ë„")
    parser.add_argument("--days", type=int, help="ìµœê·¼ Nì¼ ì´ë‚´")
    parser.add_argument("--all", action="store_true", help="configì˜ ëª¨ë“  ì €ë„ ìˆ˜ì§‘")
    args = parser.parse_args()
    
    config = load_config()
    
    if args.all:
        # ëª¨ë“  ì €ë„ ìˆ˜ì§‘
        all_articles = []
        for jkey, jinfo in config["journals"].items():
            print(f"\n{'='*50}")
            print(f"ğŸ“– {jinfo['name']}")
            print(f"{'='*50}")
            
            pmids = search_pubmed(
                jinfo["pubmed_query"],
                year=args.year or datetime.now().year,
                days=args.days
            )
            if pmids:
                articles = fetch_details(pmids)
                # ì €ë„ í‚¤ ì¶”ê°€
                for a in articles:
                    a["_journal_key"] = jkey
                save_results(articles, jkey, str(args.year or datetime.now().year))
                print_summary(articles)
                all_articles.extend(articles)
            time.sleep(1)
        
        # í†µí•© íŒŒì¼ë„ ì €ì¥
        if all_articles:
            save_results(all_articles, "all_journals", str(args.year or datetime.now().year))
        
        print(f"\nâœ… ì „ì²´ {len(all_articles)}í¸ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles
    
    elif args.journal:
        # ë‹¨ì¼ ì €ë„
        print(f"\nğŸ“– {args.journal}")
        pmids = search_pubmed(args.journal, year=args.year, days=args.days)
        if pmids:
            articles = fetch_details(pmids)
            save_results(articles, args.journal.replace(" ", "_"), str(args.year or ""))
            print_summary(articles)
            return articles
    
    else:
        # ê¸°ë³¸: configì˜ ì²« ë²ˆì§¸ ì €ë„, ì˜¬í•´
        jkey = list(config["journals"].keys())[0]
        jinfo = config["journals"][jkey]
        year = args.year or datetime.now().year
        
        print(f"\nğŸ“– {jinfo['name']} ({year})")
        pmids = search_pubmed(jinfo["pubmed_query"], year=year)
        if pmids:
            articles = fetch_details(pmids)
            for a in articles:
                a["_journal_key"] = jkey
            save_results(articles, jkey, str(year))
            print_summary(articles)
            return articles

if __name__ == "__main__":
    main()

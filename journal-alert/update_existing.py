#!/usr/bin/env python3
"""
ê¸°ì¡´ Notion í˜ì´ì§€ì— Type + í•œê¸€ ìš”ì•½/ë²ˆì—­ ì—…ë°ì´íŠ¸
Usage: python update_existing.py
"""

import json
import os
import sys
import glob
import urllib.request
import subprocess
import shutil
import time
from datetime import datetime

CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")

def load_config():
    with open(CONFIG_PATH, "r") as f:
        return json.load(f)

def notion_api(endpoint: str, data: dict, token: str, method="POST") -> dict:
    url = f"https://api.notion.com/v1/{endpoint}"
    body = json.dumps(data).encode("utf-8")
    req = urllib.request.Request(url, data=body, method=method, headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    })
    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            return json.loads(resp.read())
    except urllib.error.HTTPError as e:
        error_body = e.read().decode()
        print(f"  âŒ Notion API ì˜¤ë¥˜ ({e.code}): {error_body[:200]}")
        return None

def query_all_pages(database_id: str, token: str) -> list[dict]:
    """ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ (page_id, title, doi)"""
    pages = []
    has_more = True
    start_cursor = None

    while has_more:
        payload = {"page_size": 100}
        if start_cursor:
            payload["start_cursor"] = start_cursor

        result = notion_api(f"databases/{database_id}/query", payload, token)
        if not result:
            break

        for page in result.get("results", []):
            props = page.get("properties", {})
            # Title
            title = ""
            title_prop = props.get("Title", {}).get("title", [])
            if title_prop:
                title = title_prop[0].get("plain_text", "").strip()
            # DOI
            doi_url = props.get("DOI", {}).get("url", "") or ""

            pages.append({
                "page_id": page["id"],
                "title": title,
                "doi_url": doi_url,
            })

        has_more = result.get("has_more", False)
        start_cursor = result.get("next_cursor")

    return pages

def classify_pub_type(article: dict) -> str:
    pub_types = [pt.lower() for pt in article.get("pub_types", [])]
    if any("randomized controlled trial" in pt for pt in pub_types):
        return "RCT"
    if any("meta-analysis" in pt for pt in pub_types):
        return "Meta-analysis"
    if any("systematic review" in pt for pt in pub_types):
        return "Systematic Review"
    if any("review" in pt for pt in pub_types):
        return "Review"
    if any("editorial" in pt for pt in pub_types):
        return "Editorial"
    if any("letter" in pt for pt in pub_types):
        return "Letter to Editor"
    if any("comment" in pt for pt in pub_types):
        return "Letter to Editor"
    if any("published erratum" in pt or "erratum" in pt for pt in pub_types):
        return "Erratum"
    if any("case reports" in pt for pt in pub_types):
        return "Case Report"
    if any("observational" in pt for pt in pub_types):
        return "Observational Study"
    if any("comparative study" in pt for pt in pub_types):
        return "Comparative Study"
    if any("multicenter study" in pt for pt in pub_types):
        return "Multicenter Study"
    if any("validation study" in pt for pt in pub_types):
        return "Validation Study"
    if any("historical article" in pt for pt in pub_types):
        return "Historical Article"
    return "Clinical Study"

def call_claude(prompt: str) -> str | None:
    try:
        env = os.environ.copy()
        env.pop("CLAUDECODE", None)  # ì¤‘ì²© ì„¸ì…˜ ë°©ì§€
        result = subprocess.run(
            ["claude", "-p", "--model", "haiku", prompt],
            capture_output=True, text=True, timeout=120, env=env,
        )
        if result.returncode == 0 and result.stdout.strip():
            return result.stdout.strip()
        return None
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return None

def summarize_and_translate(title: str, abstract: str) -> tuple[str, str]:
    if not abstract:
        return "", ""

    prompt = f"""ë…¼ë¬¸ ì œëª©: {title}

Abstract:
{abstract}

ë‹¤ìŒ 2ê°€ì§€ë¥¼ ì¶œë ¥í•˜ì„¸ìš”. êµ¬ë¶„ì "---" ë¥¼ ì‚¬ì´ì— ë„£ìœ¼ì„¸ìš”.

1) ì´ ë…¼ë¬¸ì˜ ê²°ë¡ ì„ í•œê¸€ 1ì¤„ë¡œ ìš”ì•½ (50ì ë‚´ì™¸, í•µì‹¬ ìˆ˜ì¹˜ í¬í•¨). ì˜í•™ìš©ì–´ëŠ” ì˜ë¬¸ ë³‘ê¸°.
2) Abstract ì „ì²´ë¥¼ í•œê¸€ë¡œ ë²ˆì—­ (ì˜í•™ìš©ì–´ ì˜ë¬¸ ë³‘ê¸°, ì›ë¬¸ êµ¬ì¡° ìœ ì§€).

í˜•ì‹:
[1ì¤„ ìš”ì•½]
---
[í•œê¸€ ë²ˆì—­]"""

    result = call_claude(prompt)
    if not result:
        return abstract[:100] if abstract else "", ""

    parts = result.split("---", 1)
    summary = parts[0].strip()
    translation = parts[1].strip() if len(parts) > 1 else ""
    return summary, translation

def _chunk_text(text: str, size: int) -> list[str]:
    if len(text) <= size:
        return [text]
    chunks = []
    while text:
        chunks.append(text[:size])
        text = text[size:]
    return chunks

def update_page(page_id: str, article: dict, token: str, use_claude: bool) -> bool:
    """í˜ì´ì§€ properties ì—…ë°ì´íŠ¸ + í•œê¸€ ë²ˆì—­ ë¸”ë¡ ì¶”ê°€"""
    # Type ë¶„ë¥˜
    pub_type = classify_pub_type(article)

    # í•œê¸€ ìš”ì•½/ë²ˆì—­
    abstract = article.get("abstract", "")
    if use_claude and abstract:
        summary_ko, translation_ko = summarize_and_translate(article["title"], abstract)
    else:
        summary_ko = abstract[:100] if abstract else ""
        translation_ko = ""

    # Properties ì—…ë°ì´íŠ¸ (PATCH)
    props = {
        "Type": {"select": {"name": pub_type}},
    }
    # Vol / Issue
    volume = article.get("volume", "")
    issue = article.get("issue", "")
    if volume:
        props["Vol"] = {"rich_text": [{"text": {"content": volume}}]}
    if issue:
        props["Issue"] = {"rich_text": [{"text": {"content": issue}}]}

    if summary_ko:
        props["Summary"] = {"rich_text": [{"text": {"content": summary_ko[:2000]}}]}

    result = notion_api(f"pages/{page_id}", {"properties": props}, token, method="PATCH")
    if not result:
        return False

    # í•œê¸€ ë²ˆì—­ ë¸”ë¡ ì¶”ê°€
    if translation_ko:
        blocks = [
            {"object": "block", "type": "divider", "divider": {}},
            {
                "object": "block", "type": "heading_2",
                "heading_2": {"rich_text": [{"type": "text", "text": {"content": "í•œê¸€ ë²ˆì—­"}}]}
            },
        ]
        for chunk in _chunk_text(translation_ko, 2000):
            blocks.append({
                "object": "block", "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": chunk}}]}
            })

        notion_api(f"blocks/{page_id}/children", {"children": blocks}, token, method="PATCH")

    return True


def main():
    config = load_config()
    token = os.environ.get("NOTION_TOKEN") or config.get("notion_token", "")
    if not token:
        print("âŒ NOTION_TOKEN í•„ìš”")
        sys.exit(1)

    database_id = config["notion_database_id"]

    # Claude CLI í™•ì¸
    use_claude = shutil.which("claude") is not None
    if use_claude:
        print("ğŸ¤– Claude CLI ê°ì§€ â€” í•œê¸€ ìš”ì•½/ë²ˆì—­ ìƒì„±")
    else:
        print("âš ï¸  Claude CLI ì—†ìŒ â€” Typeë§Œ ì—…ë°ì´íŠ¸")

    # 1. Notion ê¸°ì¡´ í˜ì´ì§€ ì¡°íšŒ
    print("ğŸ“‹ Notion DB ì¡°íšŒ ì¤‘...")
    pages = query_all_pages(database_id, token)
    print(f"   {len(pages)}ê±´ ì¡°íšŒë¨")

    # 2. ë°ì´í„° JSON ë¡œë“œ
    files = sorted(glob.glob(os.path.join(DATA_DIR, "*.json")))
    if not files:
        print("âŒ data/ ì— JSON íŒŒì¼ ì—†ìŒ")
        sys.exit(1)

    articles = []
    with open(files[-1], "r", encoding="utf-8") as f:
        articles = json.load(f)
    print(f"ğŸ“‚ {os.path.basename(files[-1])} ({len(articles)}í¸)")

    # 3. DOI/Titleë¡œ ë§¤ì¹­ ì¸ë±ìŠ¤ êµ¬ì¶•
    by_doi = {}
    by_title = {}
    for a in articles:
        if a.get("doi_url"):
            by_doi[a["doi_url"]] = a
        if a.get("title"):
            by_title[a["title"].strip()[:50]] = a

    # 4. ì—…ë°ì´íŠ¸
    updated = 0
    skipped = 0
    failed = 0

    for i, page in enumerate(pages):
        # ë§¤ì¹­
        article = by_doi.get(page["doi_url"]) or by_title.get(page["title"][:50])
        if not article:
            skipped += 1
            continue

        pub_type = classify_pub_type(article)
        print(f"  [{i+1}/{len(pages)}] {pub_type:20s} | {page['title'][:50]}...")

        if update_page(page["page_id"], article, token, use_claude):
            updated += 1
        else:
            failed += 1

        time.sleep(0.5)  # rate limit (Claude CLI + Notion)

    print(f"\nâœ… ì™„ë£Œ: ì—…ë°ì´íŠ¸ {updated}ê±´, ë§¤ì¹­ì‹¤íŒ¨ {skipped}ê±´, ì˜¤ë¥˜ {failed}ê±´")


if __name__ == "__main__":
    main()
